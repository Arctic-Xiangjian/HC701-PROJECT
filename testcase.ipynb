{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "import timm\n",
    "import copy\n",
    "\n",
    "from HC701PROJECT.hc701fed.model.baseline import Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Baseline('resnet50', pretrained=False)\n",
    "model2 = copy.deepcopy(model1)\n",
    "model3 = timm.create_model('resnet50', pretrained=False, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([5, 2048]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([5]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/xiangjianhou/hc701-fed/testcase.ipynb Cell 3\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.79/home/xiangjianhou/hc701-fed/testcase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.127.30.79/home/xiangjianhou/hc701-fed/testcase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model3\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m/home/xiangjianhou/hc701-fed/checkpoint_simclr/model_20230416195313_199.pth\u001b[39;49m\u001b[39m'\u001b[39;49m), strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.conda/envs/NRDS/lib/python3.8/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([128, 2048]) from checkpoint, the shape in current model is torch.Size([5, 2048]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([5])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model3.load_state_dict(torch.load('/home/xiangjianhou/hc701-fed/checkpoint_simclr/model_20230416195313_199.pth'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 7, 7]) torch.Size([64, 3, 7, 7]) conv1.weight conv1.weight\n",
      "torch.Size([64]) torch.Size([64]) bn1.weight bn1.weight\n",
      "torch.Size([64]) torch.Size([64]) bn1.bias bn1.bias\n",
      "torch.Size([64]) torch.Size([64]) bn1.running_mean bn1.running_mean\n",
      "torch.Size([64]) torch.Size([64]) bn1.running_var bn1.running_var\n",
      "torch.Size([]) torch.Size([]) bn1.num_batches_tracked bn1.num_batches_tracked\n",
      "torch.Size([64, 64, 1, 1]) torch.Size([64, 64, 1, 1]) layer1.0.conv1.weight layer1.0.conv1.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.0.bn1.weight layer1.0.bn1.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.0.bn1.bias layer1.0.bn1.bias\n",
      "torch.Size([64]) torch.Size([64]) layer1.0.bn1.running_mean layer1.0.bn1.running_mean\n",
      "torch.Size([64]) torch.Size([64]) layer1.0.bn1.running_var layer1.0.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer1.0.bn1.num_batches_tracked layer1.0.bn1.num_batches_tracked\n",
      "torch.Size([64, 64, 3, 3]) torch.Size([64, 64, 3, 3]) layer1.0.conv2.weight layer1.0.conv2.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.0.bn2.weight layer1.0.bn2.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.0.bn2.bias layer1.0.bn2.bias\n",
      "torch.Size([64]) torch.Size([64]) layer1.0.bn2.running_mean layer1.0.bn2.running_mean\n",
      "torch.Size([64]) torch.Size([64]) layer1.0.bn2.running_var layer1.0.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer1.0.bn2.num_batches_tracked layer1.0.bn2.num_batches_tracked\n",
      "torch.Size([256, 64, 1, 1]) torch.Size([256, 64, 1, 1]) layer1.0.conv3.weight layer1.0.conv3.weight\n",
      "torch.Size([256]) torch.Size([256]) layer1.0.bn3.weight layer1.0.bn3.weight\n",
      "torch.Size([256]) torch.Size([256]) layer1.0.bn3.bias layer1.0.bn3.bias\n",
      "torch.Size([256]) torch.Size([256]) layer1.0.bn3.running_mean layer1.0.bn3.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer1.0.bn3.running_var layer1.0.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer1.0.bn3.num_batches_tracked layer1.0.bn3.num_batches_tracked\n",
      "torch.Size([256, 64, 1, 1]) torch.Size([256, 64, 1, 1]) layer1.0.downsample.0.weight layer1.0.downsample.0.weight\n",
      "torch.Size([256]) torch.Size([256]) layer1.0.downsample.1.weight layer1.0.downsample.1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer1.0.downsample.1.bias layer1.0.downsample.1.bias\n",
      "torch.Size([256]) torch.Size([256]) layer1.0.downsample.1.running_mean layer1.0.downsample.1.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer1.0.downsample.1.running_var layer1.0.downsample.1.running_var\n",
      "torch.Size([]) torch.Size([]) layer1.0.downsample.1.num_batches_tracked layer1.0.downsample.1.num_batches_tracked\n",
      "torch.Size([64, 256, 1, 1]) torch.Size([64, 256, 1, 1]) layer1.1.conv1.weight layer1.1.conv1.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.1.bn1.weight layer1.1.bn1.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.1.bn1.bias layer1.1.bn1.bias\n",
      "torch.Size([64]) torch.Size([64]) layer1.1.bn1.running_mean layer1.1.bn1.running_mean\n",
      "torch.Size([64]) torch.Size([64]) layer1.1.bn1.running_var layer1.1.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer1.1.bn1.num_batches_tracked layer1.1.bn1.num_batches_tracked\n",
      "torch.Size([64, 64, 3, 3]) torch.Size([64, 64, 3, 3]) layer1.1.conv2.weight layer1.1.conv2.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.1.bn2.weight layer1.1.bn2.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.1.bn2.bias layer1.1.bn2.bias\n",
      "torch.Size([64]) torch.Size([64]) layer1.1.bn2.running_mean layer1.1.bn2.running_mean\n",
      "torch.Size([64]) torch.Size([64]) layer1.1.bn2.running_var layer1.1.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer1.1.bn2.num_batches_tracked layer1.1.bn2.num_batches_tracked\n",
      "torch.Size([256, 64, 1, 1]) torch.Size([256, 64, 1, 1]) layer1.1.conv3.weight layer1.1.conv3.weight\n",
      "torch.Size([256]) torch.Size([256]) layer1.1.bn3.weight layer1.1.bn3.weight\n",
      "torch.Size([256]) torch.Size([256]) layer1.1.bn3.bias layer1.1.bn3.bias\n",
      "torch.Size([256]) torch.Size([256]) layer1.1.bn3.running_mean layer1.1.bn3.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer1.1.bn3.running_var layer1.1.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer1.1.bn3.num_batches_tracked layer1.1.bn3.num_batches_tracked\n",
      "torch.Size([64, 256, 1, 1]) torch.Size([64, 256, 1, 1]) layer1.2.conv1.weight layer1.2.conv1.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.2.bn1.weight layer1.2.bn1.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.2.bn1.bias layer1.2.bn1.bias\n",
      "torch.Size([64]) torch.Size([64]) layer1.2.bn1.running_mean layer1.2.bn1.running_mean\n",
      "torch.Size([64]) torch.Size([64]) layer1.2.bn1.running_var layer1.2.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer1.2.bn1.num_batches_tracked layer1.2.bn1.num_batches_tracked\n",
      "torch.Size([64, 64, 3, 3]) torch.Size([64, 64, 3, 3]) layer1.2.conv2.weight layer1.2.conv2.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.2.bn2.weight layer1.2.bn2.weight\n",
      "torch.Size([64]) torch.Size([64]) layer1.2.bn2.bias layer1.2.bn2.bias\n",
      "torch.Size([64]) torch.Size([64]) layer1.2.bn2.running_mean layer1.2.bn2.running_mean\n",
      "torch.Size([64]) torch.Size([64]) layer1.2.bn2.running_var layer1.2.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer1.2.bn2.num_batches_tracked layer1.2.bn2.num_batches_tracked\n",
      "torch.Size([256, 64, 1, 1]) torch.Size([256, 64, 1, 1]) layer1.2.conv3.weight layer1.2.conv3.weight\n",
      "torch.Size([256]) torch.Size([256]) layer1.2.bn3.weight layer1.2.bn3.weight\n",
      "torch.Size([256]) torch.Size([256]) layer1.2.bn3.bias layer1.2.bn3.bias\n",
      "torch.Size([256]) torch.Size([256]) layer1.2.bn3.running_mean layer1.2.bn3.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer1.2.bn3.running_var layer1.2.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer1.2.bn3.num_batches_tracked layer1.2.bn3.num_batches_tracked\n",
      "torch.Size([128, 256, 1, 1]) torch.Size([128, 256, 1, 1]) layer2.0.conv1.weight layer2.0.conv1.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.0.bn1.weight layer2.0.bn1.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.0.bn1.bias layer2.0.bn1.bias\n",
      "torch.Size([128]) torch.Size([128]) layer2.0.bn1.running_mean layer2.0.bn1.running_mean\n",
      "torch.Size([128]) torch.Size([128]) layer2.0.bn1.running_var layer2.0.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.0.bn1.num_batches_tracked layer2.0.bn1.num_batches_tracked\n",
      "torch.Size([128, 128, 3, 3]) torch.Size([128, 128, 3, 3]) layer2.0.conv2.weight layer2.0.conv2.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.0.bn2.weight layer2.0.bn2.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.0.bn2.bias layer2.0.bn2.bias\n",
      "torch.Size([128]) torch.Size([128]) layer2.0.bn2.running_mean layer2.0.bn2.running_mean\n",
      "torch.Size([128]) torch.Size([128]) layer2.0.bn2.running_var layer2.0.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.0.bn2.num_batches_tracked layer2.0.bn2.num_batches_tracked\n",
      "torch.Size([512, 128, 1, 1]) torch.Size([512, 128, 1, 1]) layer2.0.conv3.weight layer2.0.conv3.weight\n",
      "torch.Size([512]) torch.Size([512]) layer2.0.bn3.weight layer2.0.bn3.weight\n",
      "torch.Size([512]) torch.Size([512]) layer2.0.bn3.bias layer2.0.bn3.bias\n",
      "torch.Size([512]) torch.Size([512]) layer2.0.bn3.running_mean layer2.0.bn3.running_mean\n",
      "torch.Size([512]) torch.Size([512]) layer2.0.bn3.running_var layer2.0.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.0.bn3.num_batches_tracked layer2.0.bn3.num_batches_tracked\n",
      "torch.Size([512, 256, 1, 1]) torch.Size([512, 256, 1, 1]) layer2.0.downsample.0.weight layer2.0.downsample.0.weight\n",
      "torch.Size([512]) torch.Size([512]) layer2.0.downsample.1.weight layer2.0.downsample.1.weight\n",
      "torch.Size([512]) torch.Size([512]) layer2.0.downsample.1.bias layer2.0.downsample.1.bias\n",
      "torch.Size([512]) torch.Size([512]) layer2.0.downsample.1.running_mean layer2.0.downsample.1.running_mean\n",
      "torch.Size([512]) torch.Size([512]) layer2.0.downsample.1.running_var layer2.0.downsample.1.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.0.downsample.1.num_batches_tracked layer2.0.downsample.1.num_batches_tracked\n",
      "torch.Size([128, 512, 1, 1]) torch.Size([128, 512, 1, 1]) layer2.1.conv1.weight layer2.1.conv1.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.1.bn1.weight layer2.1.bn1.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.1.bn1.bias layer2.1.bn1.bias\n",
      "torch.Size([128]) torch.Size([128]) layer2.1.bn1.running_mean layer2.1.bn1.running_mean\n",
      "torch.Size([128]) torch.Size([128]) layer2.1.bn1.running_var layer2.1.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.1.bn1.num_batches_tracked layer2.1.bn1.num_batches_tracked\n",
      "torch.Size([128, 128, 3, 3]) torch.Size([128, 128, 3, 3]) layer2.1.conv2.weight layer2.1.conv2.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.1.bn2.weight layer2.1.bn2.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.1.bn2.bias layer2.1.bn2.bias\n",
      "torch.Size([128]) torch.Size([128]) layer2.1.bn2.running_mean layer2.1.bn2.running_mean\n",
      "torch.Size([128]) torch.Size([128]) layer2.1.bn2.running_var layer2.1.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.1.bn2.num_batches_tracked layer2.1.bn2.num_batches_tracked\n",
      "torch.Size([512, 128, 1, 1]) torch.Size([512, 128, 1, 1]) layer2.1.conv3.weight layer2.1.conv3.weight\n",
      "torch.Size([512]) torch.Size([512]) layer2.1.bn3.weight layer2.1.bn3.weight\n",
      "torch.Size([512]) torch.Size([512]) layer2.1.bn3.bias layer2.1.bn3.bias\n",
      "torch.Size([512]) torch.Size([512]) layer2.1.bn3.running_mean layer2.1.bn3.running_mean\n",
      "torch.Size([512]) torch.Size([512]) layer2.1.bn3.running_var layer2.1.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.1.bn3.num_batches_tracked layer2.1.bn3.num_batches_tracked\n",
      "torch.Size([128, 512, 1, 1]) torch.Size([128, 512, 1, 1]) layer2.2.conv1.weight layer2.2.conv1.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.2.bn1.weight layer2.2.bn1.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.2.bn1.bias layer2.2.bn1.bias\n",
      "torch.Size([128]) torch.Size([128]) layer2.2.bn1.running_mean layer2.2.bn1.running_mean\n",
      "torch.Size([128]) torch.Size([128]) layer2.2.bn1.running_var layer2.2.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.2.bn1.num_batches_tracked layer2.2.bn1.num_batches_tracked\n",
      "torch.Size([128, 128, 3, 3]) torch.Size([128, 128, 3, 3]) layer2.2.conv2.weight layer2.2.conv2.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.2.bn2.weight layer2.2.bn2.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.2.bn2.bias layer2.2.bn2.bias\n",
      "torch.Size([128]) torch.Size([128]) layer2.2.bn2.running_mean layer2.2.bn2.running_mean\n",
      "torch.Size([128]) torch.Size([128]) layer2.2.bn2.running_var layer2.2.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.2.bn2.num_batches_tracked layer2.2.bn2.num_batches_tracked\n",
      "torch.Size([512, 128, 1, 1]) torch.Size([512, 128, 1, 1]) layer2.2.conv3.weight layer2.2.conv3.weight\n",
      "torch.Size([512]) torch.Size([512]) layer2.2.bn3.weight layer2.2.bn3.weight\n",
      "torch.Size([512]) torch.Size([512]) layer2.2.bn3.bias layer2.2.bn3.bias\n",
      "torch.Size([512]) torch.Size([512]) layer2.2.bn3.running_mean layer2.2.bn3.running_mean\n",
      "torch.Size([512]) torch.Size([512]) layer2.2.bn3.running_var layer2.2.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.2.bn3.num_batches_tracked layer2.2.bn3.num_batches_tracked\n",
      "torch.Size([128, 512, 1, 1]) torch.Size([128, 512, 1, 1]) layer2.3.conv1.weight layer2.3.conv1.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.3.bn1.weight layer2.3.bn1.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.3.bn1.bias layer2.3.bn1.bias\n",
      "torch.Size([128]) torch.Size([128]) layer2.3.bn1.running_mean layer2.3.bn1.running_mean\n",
      "torch.Size([128]) torch.Size([128]) layer2.3.bn1.running_var layer2.3.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.3.bn1.num_batches_tracked layer2.3.bn1.num_batches_tracked\n",
      "torch.Size([128, 128, 3, 3]) torch.Size([128, 128, 3, 3]) layer2.3.conv2.weight layer2.3.conv2.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.3.bn2.weight layer2.3.bn2.weight\n",
      "torch.Size([128]) torch.Size([128]) layer2.3.bn2.bias layer2.3.bn2.bias\n",
      "torch.Size([128]) torch.Size([128]) layer2.3.bn2.running_mean layer2.3.bn2.running_mean\n",
      "torch.Size([128]) torch.Size([128]) layer2.3.bn2.running_var layer2.3.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.3.bn2.num_batches_tracked layer2.3.bn2.num_batches_tracked\n",
      "torch.Size([512, 128, 1, 1]) torch.Size([512, 128, 1, 1]) layer2.3.conv3.weight layer2.3.conv3.weight\n",
      "torch.Size([512]) torch.Size([512]) layer2.3.bn3.weight layer2.3.bn3.weight\n",
      "torch.Size([512]) torch.Size([512]) layer2.3.bn3.bias layer2.3.bn3.bias\n",
      "torch.Size([512]) torch.Size([512]) layer2.3.bn3.running_mean layer2.3.bn3.running_mean\n",
      "torch.Size([512]) torch.Size([512]) layer2.3.bn3.running_var layer2.3.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer2.3.bn3.num_batches_tracked layer2.3.bn3.num_batches_tracked\n",
      "torch.Size([256, 512, 1, 1]) torch.Size([256, 512, 1, 1]) layer3.0.conv1.weight layer3.0.conv1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.0.bn1.weight layer3.0.bn1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.0.bn1.bias layer3.0.bn1.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.0.bn1.running_mean layer3.0.bn1.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.0.bn1.running_var layer3.0.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.0.bn1.num_batches_tracked layer3.0.bn1.num_batches_tracked\n",
      "torch.Size([256, 256, 3, 3]) torch.Size([256, 256, 3, 3]) layer3.0.conv2.weight layer3.0.conv2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.0.bn2.weight layer3.0.bn2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.0.bn2.bias layer3.0.bn2.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.0.bn2.running_mean layer3.0.bn2.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.0.bn2.running_var layer3.0.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.0.bn2.num_batches_tracked layer3.0.bn2.num_batches_tracked\n",
      "torch.Size([1024, 256, 1, 1]) torch.Size([1024, 256, 1, 1]) layer3.0.conv3.weight layer3.0.conv3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.0.bn3.weight layer3.0.bn3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.0.bn3.bias layer3.0.bn3.bias\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.0.bn3.running_mean layer3.0.bn3.running_mean\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.0.bn3.running_var layer3.0.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.0.bn3.num_batches_tracked layer3.0.bn3.num_batches_tracked\n",
      "torch.Size([1024, 512, 1, 1]) torch.Size([1024, 512, 1, 1]) layer3.0.downsample.0.weight layer3.0.downsample.0.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.0.downsample.1.weight layer3.0.downsample.1.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.0.downsample.1.bias layer3.0.downsample.1.bias\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.0.downsample.1.running_mean layer3.0.downsample.1.running_mean\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.0.downsample.1.running_var layer3.0.downsample.1.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.0.downsample.1.num_batches_tracked layer3.0.downsample.1.num_batches_tracked\n",
      "torch.Size([256, 1024, 1, 1]) torch.Size([256, 1024, 1, 1]) layer3.1.conv1.weight layer3.1.conv1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.1.bn1.weight layer3.1.bn1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.1.bn1.bias layer3.1.bn1.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.1.bn1.running_mean layer3.1.bn1.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.1.bn1.running_var layer3.1.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.1.bn1.num_batches_tracked layer3.1.bn1.num_batches_tracked\n",
      "torch.Size([256, 256, 3, 3]) torch.Size([256, 256, 3, 3]) layer3.1.conv2.weight layer3.1.conv2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.1.bn2.weight layer3.1.bn2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.1.bn2.bias layer3.1.bn2.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.1.bn2.running_mean layer3.1.bn2.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.1.bn2.running_var layer3.1.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.1.bn2.num_batches_tracked layer3.1.bn2.num_batches_tracked\n",
      "torch.Size([1024, 256, 1, 1]) torch.Size([1024, 256, 1, 1]) layer3.1.conv3.weight layer3.1.conv3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.1.bn3.weight layer3.1.bn3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.1.bn3.bias layer3.1.bn3.bias\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.1.bn3.running_mean layer3.1.bn3.running_mean\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.1.bn3.running_var layer3.1.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.1.bn3.num_batches_tracked layer3.1.bn3.num_batches_tracked\n",
      "torch.Size([256, 1024, 1, 1]) torch.Size([256, 1024, 1, 1]) layer3.2.conv1.weight layer3.2.conv1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.2.bn1.weight layer3.2.bn1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.2.bn1.bias layer3.2.bn1.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.2.bn1.running_mean layer3.2.bn1.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.2.bn1.running_var layer3.2.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.2.bn1.num_batches_tracked layer3.2.bn1.num_batches_tracked\n",
      "torch.Size([256, 256, 3, 3]) torch.Size([256, 256, 3, 3]) layer3.2.conv2.weight layer3.2.conv2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.2.bn2.weight layer3.2.bn2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.2.bn2.bias layer3.2.bn2.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.2.bn2.running_mean layer3.2.bn2.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.2.bn2.running_var layer3.2.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.2.bn2.num_batches_tracked layer3.2.bn2.num_batches_tracked\n",
      "torch.Size([1024, 256, 1, 1]) torch.Size([1024, 256, 1, 1]) layer3.2.conv3.weight layer3.2.conv3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.2.bn3.weight layer3.2.bn3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.2.bn3.bias layer3.2.bn3.bias\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.2.bn3.running_mean layer3.2.bn3.running_mean\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.2.bn3.running_var layer3.2.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.2.bn3.num_batches_tracked layer3.2.bn3.num_batches_tracked\n",
      "torch.Size([256, 1024, 1, 1]) torch.Size([256, 1024, 1, 1]) layer3.3.conv1.weight layer3.3.conv1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.3.bn1.weight layer3.3.bn1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.3.bn1.bias layer3.3.bn1.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.3.bn1.running_mean layer3.3.bn1.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.3.bn1.running_var layer3.3.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.3.bn1.num_batches_tracked layer3.3.bn1.num_batches_tracked\n",
      "torch.Size([256, 256, 3, 3]) torch.Size([256, 256, 3, 3]) layer3.3.conv2.weight layer3.3.conv2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.3.bn2.weight layer3.3.bn2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.3.bn2.bias layer3.3.bn2.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.3.bn2.running_mean layer3.3.bn2.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.3.bn2.running_var layer3.3.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.3.bn2.num_batches_tracked layer3.3.bn2.num_batches_tracked\n",
      "torch.Size([1024, 256, 1, 1]) torch.Size([1024, 256, 1, 1]) layer3.3.conv3.weight layer3.3.conv3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.3.bn3.weight layer3.3.bn3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.3.bn3.bias layer3.3.bn3.bias\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.3.bn3.running_mean layer3.3.bn3.running_mean\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.3.bn3.running_var layer3.3.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.3.bn3.num_batches_tracked layer3.3.bn3.num_batches_tracked\n",
      "torch.Size([256, 1024, 1, 1]) torch.Size([256, 1024, 1, 1]) layer3.4.conv1.weight layer3.4.conv1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.4.bn1.weight layer3.4.bn1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.4.bn1.bias layer3.4.bn1.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.4.bn1.running_mean layer3.4.bn1.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.4.bn1.running_var layer3.4.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.4.bn1.num_batches_tracked layer3.4.bn1.num_batches_tracked\n",
      "torch.Size([256, 256, 3, 3]) torch.Size([256, 256, 3, 3]) layer3.4.conv2.weight layer3.4.conv2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.4.bn2.weight layer3.4.bn2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.4.bn2.bias layer3.4.bn2.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.4.bn2.running_mean layer3.4.bn2.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.4.bn2.running_var layer3.4.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.4.bn2.num_batches_tracked layer3.4.bn2.num_batches_tracked\n",
      "torch.Size([1024, 256, 1, 1]) torch.Size([1024, 256, 1, 1]) layer3.4.conv3.weight layer3.4.conv3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.4.bn3.weight layer3.4.bn3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.4.bn3.bias layer3.4.bn3.bias\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.4.bn3.running_mean layer3.4.bn3.running_mean\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.4.bn3.running_var layer3.4.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.4.bn3.num_batches_tracked layer3.4.bn3.num_batches_tracked\n",
      "torch.Size([256, 1024, 1, 1]) torch.Size([256, 1024, 1, 1]) layer3.5.conv1.weight layer3.5.conv1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.5.bn1.weight layer3.5.bn1.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.5.bn1.bias layer3.5.bn1.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.5.bn1.running_mean layer3.5.bn1.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.5.bn1.running_var layer3.5.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.5.bn1.num_batches_tracked layer3.5.bn1.num_batches_tracked\n",
      "torch.Size([256, 256, 3, 3]) torch.Size([256, 256, 3, 3]) layer3.5.conv2.weight layer3.5.conv2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.5.bn2.weight layer3.5.bn2.weight\n",
      "torch.Size([256]) torch.Size([256]) layer3.5.bn2.bias layer3.5.bn2.bias\n",
      "torch.Size([256]) torch.Size([256]) layer3.5.bn2.running_mean layer3.5.bn2.running_mean\n",
      "torch.Size([256]) torch.Size([256]) layer3.5.bn2.running_var layer3.5.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.5.bn2.num_batches_tracked layer3.5.bn2.num_batches_tracked\n",
      "torch.Size([1024, 256, 1, 1]) torch.Size([1024, 256, 1, 1]) layer3.5.conv3.weight layer3.5.conv3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.5.bn3.weight layer3.5.bn3.weight\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.5.bn3.bias layer3.5.bn3.bias\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.5.bn3.running_mean layer3.5.bn3.running_mean\n",
      "torch.Size([1024]) torch.Size([1024]) layer3.5.bn3.running_var layer3.5.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer3.5.bn3.num_batches_tracked layer3.5.bn3.num_batches_tracked\n",
      "torch.Size([512, 1024, 1, 1]) torch.Size([512, 1024, 1, 1]) layer4.0.conv1.weight layer4.0.conv1.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.0.bn1.weight layer4.0.bn1.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.0.bn1.bias layer4.0.bn1.bias\n",
      "torch.Size([512]) torch.Size([512]) layer4.0.bn1.running_mean layer4.0.bn1.running_mean\n",
      "torch.Size([512]) torch.Size([512]) layer4.0.bn1.running_var layer4.0.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer4.0.bn1.num_batches_tracked layer4.0.bn1.num_batches_tracked\n",
      "torch.Size([512, 512, 3, 3]) torch.Size([512, 512, 3, 3]) layer4.0.conv2.weight layer4.0.conv2.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.0.bn2.weight layer4.0.bn2.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.0.bn2.bias layer4.0.bn2.bias\n",
      "torch.Size([512]) torch.Size([512]) layer4.0.bn2.running_mean layer4.0.bn2.running_mean\n",
      "torch.Size([512]) torch.Size([512]) layer4.0.bn2.running_var layer4.0.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer4.0.bn2.num_batches_tracked layer4.0.bn2.num_batches_tracked\n",
      "torch.Size([2048, 512, 1, 1]) torch.Size([2048, 512, 1, 1]) layer4.0.conv3.weight layer4.0.conv3.weight\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.0.bn3.weight layer4.0.bn3.weight\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.0.bn3.bias layer4.0.bn3.bias\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.0.bn3.running_mean layer4.0.bn3.running_mean\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.0.bn3.running_var layer4.0.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer4.0.bn3.num_batches_tracked layer4.0.bn3.num_batches_tracked\n",
      "torch.Size([2048, 1024, 1, 1]) torch.Size([2048, 1024, 1, 1]) layer4.0.downsample.0.weight layer4.0.downsample.0.weight\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.0.downsample.1.weight layer4.0.downsample.1.weight\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.0.downsample.1.bias layer4.0.downsample.1.bias\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.0.downsample.1.running_mean layer4.0.downsample.1.running_mean\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.0.downsample.1.running_var layer4.0.downsample.1.running_var\n",
      "torch.Size([]) torch.Size([]) layer4.0.downsample.1.num_batches_tracked layer4.0.downsample.1.num_batches_tracked\n",
      "torch.Size([512, 2048, 1, 1]) torch.Size([512, 2048, 1, 1]) layer4.1.conv1.weight layer4.1.conv1.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.1.bn1.weight layer4.1.bn1.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.1.bn1.bias layer4.1.bn1.bias\n",
      "torch.Size([512]) torch.Size([512]) layer4.1.bn1.running_mean layer4.1.bn1.running_mean\n",
      "torch.Size([512]) torch.Size([512]) layer4.1.bn1.running_var layer4.1.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer4.1.bn1.num_batches_tracked layer4.1.bn1.num_batches_tracked\n",
      "torch.Size([512, 512, 3, 3]) torch.Size([512, 512, 3, 3]) layer4.1.conv2.weight layer4.1.conv2.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.1.bn2.weight layer4.1.bn2.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.1.bn2.bias layer4.1.bn2.bias\n",
      "torch.Size([512]) torch.Size([512]) layer4.1.bn2.running_mean layer4.1.bn2.running_mean\n",
      "torch.Size([512]) torch.Size([512]) layer4.1.bn2.running_var layer4.1.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer4.1.bn2.num_batches_tracked layer4.1.bn2.num_batches_tracked\n",
      "torch.Size([2048, 512, 1, 1]) torch.Size([2048, 512, 1, 1]) layer4.1.conv3.weight layer4.1.conv3.weight\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.1.bn3.weight layer4.1.bn3.weight\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.1.bn3.bias layer4.1.bn3.bias\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.1.bn3.running_mean layer4.1.bn3.running_mean\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.1.bn3.running_var layer4.1.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer4.1.bn3.num_batches_tracked layer4.1.bn3.num_batches_tracked\n",
      "torch.Size([512, 2048, 1, 1]) torch.Size([512, 2048, 1, 1]) layer4.2.conv1.weight layer4.2.conv1.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.2.bn1.weight layer4.2.bn1.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.2.bn1.bias layer4.2.bn1.bias\n",
      "torch.Size([512]) torch.Size([512]) layer4.2.bn1.running_mean layer4.2.bn1.running_mean\n",
      "torch.Size([512]) torch.Size([512]) layer4.2.bn1.running_var layer4.2.bn1.running_var\n",
      "torch.Size([]) torch.Size([]) layer4.2.bn1.num_batches_tracked layer4.2.bn1.num_batches_tracked\n",
      "torch.Size([512, 512, 3, 3]) torch.Size([512, 512, 3, 3]) layer4.2.conv2.weight layer4.2.conv2.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.2.bn2.weight layer4.2.bn2.weight\n",
      "torch.Size([512]) torch.Size([512]) layer4.2.bn2.bias layer4.2.bn2.bias\n",
      "torch.Size([512]) torch.Size([512]) layer4.2.bn2.running_mean layer4.2.bn2.running_mean\n",
      "torch.Size([512]) torch.Size([512]) layer4.2.bn2.running_var layer4.2.bn2.running_var\n",
      "torch.Size([]) torch.Size([]) layer4.2.bn2.num_batches_tracked layer4.2.bn2.num_batches_tracked\n",
      "torch.Size([2048, 512, 1, 1]) torch.Size([2048, 512, 1, 1]) layer4.2.conv3.weight layer4.2.conv3.weight\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.2.bn3.weight layer4.2.bn3.weight\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.2.bn3.bias layer4.2.bn3.bias\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.2.bn3.running_mean layer4.2.bn3.running_mean\n",
      "torch.Size([2048]) torch.Size([2048]) layer4.2.bn3.running_var layer4.2.bn3.running_var\n",
      "torch.Size([]) torch.Size([]) layer4.2.bn3.num_batches_tracked layer4.2.bn3.num_batches_tracked\n",
      "torch.Size([128, 2048]) torch.Size([128, 2048]) fc.weight fc.weight\n",
      "torch.Size([128]) torch.Size([128]) fc.bias fc.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# model2.load_state_dict(torch.load('/home/xiangjianhou/hc701-fed/checkpoint_simclr/model_20230416032207_199.pth'),strict=False)\n",
    "values_pretrained = torch.load('/home/xiangjianhou/hc701-fed/checkpoint_simclr/model_20230416195313_199.pth')\n",
    "for p1,p2,k1,k2 in zip(model3.state_dict().values(),values_pretrained.values(),model3.state_dict().keys(),values_pretrained.keys()):\n",
    "    print(p1.shape,p2.shape,k1,k2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
    "    assert p1.shape == p2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "from VAL import test\n",
    "\n",
    "from hc701fed.dataset.WeightedConcatDataset import WeightedConcatDataset\n",
    "from hc701fed.dataset.dataset_list_transform import (\n",
    "    APTOS_train,\n",
    "    EyePACS_train,\n",
    "    MESSIDOR_2_train,\n",
    "    MESSIDOR_pairs_train,\n",
    "    MESSIDOR_Etienne_train,\n",
    "    MESSIDOR_Brest_train,\n",
    ")\n",
    "\n",
    "from hc701fed.dataset.val_dataset_list import (\n",
    "    APTOS_Val,\n",
    "    EyePACS_Val,\n",
    "    MESSIDOR_2_Val,\n",
    "    MESSIDOR_pairs_Val,\n",
    "    MESSIDOR_Etienne_Val,\n",
    "    MESSIDOR_Brest_Val,\n",
    ")\n",
    "from hc701fed.dataset.val_dataset_list import (\n",
    "    APTOS_Test,\n",
    "    EyePACS_Test,\n",
    "    MESSIDOR_2_Test,\n",
    "    MESSIDOR_pairs_Test,\n",
    "    MESSIDOR_Etienne_Test,\n",
    "    MESSIDOR_Brest_Test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_dataset = ConcatDataset([APTOS_Test, EyePACS_Test, MESSIDOR_2_Test, MESSIDOR_pairs_Test, MESSIDOR_Etienne_Test, MESSIDOR_Brest_Test])\n",
    "all_val_dataset = ConcatDataset([APTOS_Val, EyePACS_Val, MESSIDOR_2_Val, MESSIDOR_pairs_Val, MESSIDOR_Etienne_Val, MESSIDOR_Brest_Val])\n",
    "\n",
    "all_test_loader = DataLoader(all_test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "all_val_loader = DataLoader(all_val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  timm.create_model('resnet50', pretrained=False, num_classes=5)\n",
    "model.load_state_dict(torch.load('/home/xiangjianhou/hc701-fed/checkpoint_simclr_fineturn/centerlized_resnet50_42/20230416_214252/centerlized_resnet50_29_20230416_214252.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 304/304 [00:09<00:00, 32.05it/s]\n"
     ]
    }
   ],
   "source": [
    "val_acc,val_f1 = test(model, 'cuda', all_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309/309 [00:09<00:00, 32.06it/s]\n"
     ]
    }
   ],
   "source": [
    "test_acc,test_f1 = test(model, 'cuda', all_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7954428291576451 0.5549775020862064 0.7881424524484014 0.5429401099336415\n"
     ]
    }
   ],
   "source": [
    "print(val_acc,val_f1,test_acc,test_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NRDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
